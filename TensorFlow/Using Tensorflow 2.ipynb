{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist_data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Datasets.index>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_data.count\n",
    "mnist_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "tf.reset_default_graph() # This is need to reset to any memorized variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, input_size], name='inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, output_size], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:0\n",
      "targets:0\n"
     ]
    }
   ],
   "source": [
    "print(inputs.name)\n",
    "print(targets.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_1 = tf.get_variable('weights_1', [input_size, hidden_layer_size])\n",
    "biases_1 = tf.get_variable('biases_1', [hidden_layer_size])\n",
    "\n",
    "outputs_1 = tf.nn.sigmoid(tf.matmul(inputs, weights_1) + biases_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_2 = tf.get_variable('weights_2', [hidden_layer_size, hidden_layer_size])\n",
    "biases_2 = tf.get_variable('biases_2', [hidden_layer_size])\n",
    "\n",
    "outputs_2 = tf.nn.sigmoid(tf.matmul(outputs_1, weights_2) + biases_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_3 = tf.get_variable('weights_3', [hidden_layer_size, output_size])\n",
    "biases_3 = tf.get_variable('biases_3', [output_size])\n",
    "\n",
    "outputs_3 = tf.matmul(outputs_2, weights_3) + biases_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=outputs_3, labels=targets)\n",
    "mean_loss = tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimize = tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_equals_target = tf.equal(tf.argmax(outputs_3, 1), tf.argmax(targets, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(out_equals_target, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "batch_numbers = mnist_data.train._num_examples // batch_size\n",
    "max_epochs = 15\n",
    "prev_validation_loss = 9999999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss 0.720\n",
      "Validation Loss 0.284\n",
      "Validation accuracy 92.560 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2\n",
      "Training Loss 0.252\n",
      "Validation Loss 0.197\n",
      "Validation accuracy 94.200 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 3\n",
      "Training Loss 0.189\n",
      "Validation Loss 0.161\n",
      "Validation accuracy 95.500 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4\n",
      "Training Loss 0.152\n",
      "Validation Loss 0.134\n",
      "Validation accuracy 96.220 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5\n",
      "Training Loss 0.126\n",
      "Validation Loss 0.123\n",
      "Validation accuracy 96.460 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6\n",
      "Training Loss 0.105\n",
      "Validation Loss 0.106\n",
      "Validation accuracy 96.860 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 7\n",
      "Training Loss 0.090\n",
      "Validation Loss 0.098\n",
      "Validation accuracy 97.160 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 8\n",
      "Training Loss 0.077\n",
      "Validation Loss 0.093\n",
      "Validation accuracy 97.180 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 9\n",
      "Training Loss 0.067\n",
      "Validation Loss 0.085\n",
      "Validation accuracy 97.420 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 10\n",
      "Training Loss 0.057\n",
      "Validation Loss 0.084\n",
      "Validation accuracy 97.320 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 11\n",
      "Training Loss 0.049\n",
      "Validation Loss 0.081\n",
      "Validation accuracy 97.460 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 12\n",
      "Training Loss 0.043\n",
      "Validation Loss 0.078\n",
      "Validation accuracy 97.680 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 13\n",
      "Training Loss 0.037\n",
      "Validation Loss 0.076\n",
      "Validation accuracy 97.640 percent\n",
      "\n",
      "\n",
      "\n",
      "Epoch 14\n",
      "Training Loss 0.032\n",
      "Validation Loss 0.077\n",
      "Validation accuracy 97.580 percent\n",
      "\n",
      "\n",
      "\n",
      "End of Training\n"
     ]
    }
   ],
   "source": [
    "for epoch_counter in range(max_epochs):\n",
    "    curr_epoch_loss = 0.0\n",
    "    for batch_counter in range(batch_numbers):\n",
    "        input_batch, target_batch = mnist_data.train.next_batch(batch_size) # Here we get in dataset the batch of size X with inputs and targets\n",
    "        \n",
    "        _, batch_loss = sess.run([optimize, mean_loss],\n",
    "                                feed_dict={inputs: input_batch, targets: target_batch})\n",
    "        \n",
    "        curr_epoch_loss += batch_loss # Sum the errors per batch\n",
    "    \n",
    "    curr_epoch_loss /= batch_numbers # Mean loss for the batchs\n",
    "    \n",
    "    #Validation part\n",
    "    input_batch, target_batch = mnist_data.validation.next_batch(mnist_data.validation._num_examples)\n",
    "    validation_loss, validation_accuracy = sess.run([mean_loss, accuracy], \n",
    "                                                   feed_dict={inputs: input_batch, targets: target_batch})\n",
    "    \n",
    "    print(\"Epoch \" + str(epoch_counter + 1))\n",
    "    print(\"Training Loss %.3f\" % curr_epoch_loss)\n",
    "    print(\"Validation Loss %.3f\" % validation_loss)\n",
    "    print(\"Validation accuracy %.3f percent\" % (validation_accuracy * 100))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    if validation_loss > prev_validation_loss:\n",
    "        break\n",
    "        \n",
    "    prev_validation_loss = validation_loss\n",
    "    \n",
    "print(\"End of Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97600001]\n",
      "The accuracy from your test model is 97.60\n"
     ]
    }
   ],
   "source": [
    "#Test Model\n",
    "input_batch, target_batch = mnist_data.test.next_batch(mnist_data.test._num_examples)\n",
    "test_accuracy = sess.run([accuracy], feed_dict={inputs: input_batch, targets: target_batch})\n",
    "\n",
    "print(test_accuracy)\n",
    "\n",
    "print(\"The accuracy from your test model is %.2f\" % (test_accuracy[0] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
